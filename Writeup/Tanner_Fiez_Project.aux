\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{DBLP:journals/corr/BrockmanCPSSTZ16}
\citation{DBLP:journals/corr/ShenTSO13}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{donovan2014new}
\citation{sutton1998reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {2}Markov Decision Process}{2}{section.2}}
\newlabel{sec:mdp}{{2}{2}{Markov Decision Process}{section.2}{}}
\newlabel{eq:dynamics}{{2}{2}{Markov Decision Process}{equation.2.2}{}}
\citation{bellman2013dynamic}
\@writefile{toc}{\contentsline {section}{\numberline {3}Model Based Reinforcement Learning}{3}{section.3}}
\newlabel{sec:model_based}{{3}{3}{Model Based Reinforcement Learning}{section.3}{}}
\newlabel{eq:inter_v}{{8}{3}{Model Based Reinforcement Learning}{equation.3.8}{}}
\newlabel{eq:inter_q}{{9}{3}{Model Based Reinforcement Learning}{equation.3.9}{}}
\newlabel{eq:evaluation}{{10}{3}{Model Based Reinforcement Learning}{equation.3.10}{}}
\newlabel{eq:v_opt}{{13}{3}{Model Based Reinforcement Learning}{equation.3.13}{}}
\newlabel{eq:q_opt}{{14}{3}{Model Based Reinforcement Learning}{equation.3.14}{}}
\newlabel{eq:improvement}{{15}{4}{Model Based Reinforcement Learning}{equation.3.15}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:value_iteration_simple}{{1a}{4}{Subfigure 1a}{subfigure.1.1}{}}
\newlabel{sub@fig:value_iteration_simple}{{(a)}{a}{Subfigure 1a\relax }{subfigure.1.1}{}}
\newlabel{fig:q_value_iteration_simple}{{1b}{4}{Subfigure 1b}{subfigure.1.2}{}}
\newlabel{sub@fig:q_value_iteration_simple}{{(b)}{b}{Subfigure 1b\relax }{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Model based algorithms in grid-world.\relax }}{4}{figure.caption.2}}
\newlabel{fig:model}{{1}{4}{Model based algorithms in grid-world.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Model Free Reinforcement Learning}{5}{section.4}}
\newlabel{sec:model_free}{{4}{5}{Model Free Reinforcement Learning}{section.4}{}}
\newlabel{eq:td_v}{{16}{5}{Model Free Reinforcement Learning}{equation.4.16}{}}
\newlabel{eq:td_q}{{17}{5}{Model Free Reinforcement Learning}{equation.4.17}{}}
\citation{robbins1951stochastic}
\citation{tsitsiklis1994asynchronous}
\citation{sutton1998reinforcement}
\citation{DBLP:journals/corr/ShenTSO13}
\citation{tversky1992advances}
\citation{DBLP:journals/corr/RatliffM17}
\@writefile{toc}{\contentsline {section}{\numberline {5}Risk-Sensitive Reinforcement Learning}{6}{section.5}}
\newlabel{sec:risk}{{5}{6}{Risk-Sensitive Reinforcement Learning}{section.5}{}}
\citation{donovan2014new}
\newlabel{fig:risk1}{{2a}{7}{Subfigure 2a}{subfigure.2.1}{}}
\newlabel{sub@fig:risk1}{{(a)}{a}{Subfigure 2a\relax }{subfigure.2.1}{}}
\newlabel{fig:risk2}{{2b}{7}{Subfigure 2b}{subfigure.2.2}{}}
\newlabel{sub@fig:risk2}{{(b)}{b}{Subfigure 2b\relax }{subfigure.2.2}{}}
\newlabel{fig:risk3}{{2c}{7}{Subfigure 2c}{subfigure.2.3}{}}
\newlabel{sub@fig:risk3}{{(c)}{c}{Subfigure 2c\relax }{subfigure.2.3}{}}
\newlabel{fig:risk4}{{2d}{7}{Subfigure 2d}{subfigure.2.4}{}}
\newlabel{sub@fig:risk4}{{(d)}{d}{Subfigure 2d\relax }{subfigure.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Prospect Theory value functions with varying risk-sensitivity parameters. In particular we fix ($c_+=1, \rho +=.5, \rho _-=.5$) and vary $c_- \in \{1, 7.5, 17.5, 20\}$ to produce the functions from left to right.\relax }}{7}{figure.caption.3}}
\newlabel{fig:prospect}{{2}{7}{Prospect Theory value functions with varying risk-sensitivity parameters. In particular we fix ($c_+=1, \rho +=.5, \rho _-=.5$) and vary $c_- \in \{1, 7.5, 17.5, 20\}$ to produce the functions from left to right.\relax }{figure.caption.3}{}}
\newlabel{fig:risk1_}{{3a}{7}{Subfigure 3a}{subfigure.3.1}{}}
\newlabel{sub@fig:risk1_}{{(a)}{a}{Subfigure 3a\relax }{subfigure.3.1}{}}
\newlabel{fig:risk2_}{{3b}{7}{Subfigure 3b}{subfigure.3.2}{}}
\newlabel{sub@fig:risk2_}{{(b)}{b}{Subfigure 3b\relax }{subfigure.3.2}{}}
\newlabel{fig:risk3_}{{3c}{7}{Subfigure 3c}{subfigure.3.3}{}}
\newlabel{sub@fig:risk3_}{{(c)}{c}{Subfigure 3c\relax }{subfigure.3.3}{}}
\newlabel{fig:risk4_}{{3d}{7}{Subfigure 3d}{subfigure.3.4}{}}
\newlabel{sub@fig:risk4_}{{(d)}{d}{Subfigure 3d\relax }{subfigure.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Risk-Sensitive $q$-learning results with varying risk-sensitivity parameters.\relax }}{7}{figure.caption.4}}
\newlabel{fig:risk}{{3}{7}{Risk-Sensitive $q$-learning results with varying risk-sensitivity parameters.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}New York Taxi Dataset}{7}{section.6}}
\newlabel{sec:taxi}{{6}{7}{New York Taxi Dataset}{section.6}{}}
\bibstyle{unsrtnat}
\bibdata{bibtex}
\bibcite{DBLP:journals/corr/BrockmanCPSSTZ16}{{1}{2016}{{Brockman et~al.}}{{Brockman, Cheung, Pettersson, Schneider, Schulman, Tang, and Zaremba}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Percentage of decision states (empty to full and not in a terminal state)\relax }}{8}{table.caption.5}}
\newlabel{my-label}{{1}{8}{Percentage of decision states (empty to full and not in a terminal state)\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions and Future Work}{8}{section.7}}
\bibcite{DBLP:journals/corr/ShenTSO13}{{2}{2013}{{Shen et~al.}}{{Shen, Tobia, Sommer, and Obermayer}}}
\bibcite{donovan2014new}{{3}{2014}{{Donovan and Work}}{{}}}
\bibcite{sutton1998reinforcement}{{4}{1998}{{Sutton and Barto}}{{}}}
\bibcite{bellman2013dynamic}{{5}{2013}{{Bellman}}{{}}}
\bibcite{robbins1951stochastic}{{6}{1951}{{Robbins and Monro}}{{}}}
\bibcite{tsitsiklis1994asynchronous}{{7}{1994}{{Tsitsiklis}}{{}}}
\bibcite{tversky1992advances}{{8}{1992}{{Tversky and Kahneman}}{{}}}
\bibcite{DBLP:journals/corr/RatliffM17}{{9}{2017}{{Ratliff and Mazumdar}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Implementation Details}{10}{section.1}}
\newlabel{imp_appendix}{{A}{10}{Implementation Details}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Risk-Sensitive Decision Making Example}{11}{section.2}}
\newlabel{example}{{B}{11}{Risk-Sensitive Decision Making Example}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Bellman Optimality Conditions Derivation}{11}{section.3}}
\newlabel{bellman}{{C}{11}{Bellman Optimality Conditions Derivation}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Model Based RL Algorithms}{12}{section.4}}
\newlabel{sec:model_appendix}{{D}{12}{Model Based RL Algorithms}{section.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Policy Iteration\relax }}{12}{algorithm.1}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Value Iteration\relax }}{13}{algorithm.2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Q Value Iteration\relax }}{13}{algorithm.3}}
\newlabel{euclid}{{3}{13}{Q Value Iteration\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}Model Free RL Algorithms}{13}{section.5}}
\newlabel{sec:model_free_appendix}{{E}{13}{Model Free RL Algorithms}{section.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces TD(0)\relax }}{13}{algorithm.4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Sarsa\relax }}{13}{algorithm.5}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Q-Learning\relax }}{14}{algorithm.6}}
\@writefile{toc}{\contentsline {section}{\numberline {F}Risk-Sensitive Reinforcement Learning}{14}{section.6}}
\newlabel{sec:risk_appendix}{{F}{14}{Risk-Sensitive Reinforcement Learning}{section.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Risk-Sensitive Q-Learning\relax }}{14}{algorithm.7}}
\@writefile{toc}{\contentsline {section}{\numberline {G}Taxi MDP Model Description}{14}{section.7}}
\newlabel{mdp}{{G}{14}{Taxi MDP Model Description}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.1}State Space}{14}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.2}Action Space}{15}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.3}Transition Kernel}{15}{subsection.7.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {G.4}Reward Function}{15}{subsection.7.4}}
