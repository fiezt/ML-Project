{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.path as mplPath\n",
    "import pickle\n",
    "import sys\n",
    "import itertools\n",
    "import gmplot\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict \n",
    "from geojson import MultiPolygon\n",
    "from shapely.geometry import Polygon\n",
    "from plotly.graph_objs import Scattermapbox, Marker, Layout, Data\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "\n",
    "    def __init__(self, driver_id, taxi_data, ride_areas, neighborhoods, \n",
    "                 interval_size, change_pairs, aggregate=False):\n",
    "        \"\"\"Initializing the MDP to have needed statistics.\n",
    "        \n",
    "        :param driver_id: Hack license from the data for the specified driver \n",
    "        or list of hack licenses for a group of drivers.\n",
    "        :param ride_areas: list of integer ride keys that correspond to a \n",
    "        neighborhood in New York.\n",
    "        :param neighborhoods: list of numpy arrays of the polygon of coordinates\n",
    "        that define a neighborhood. Each array is a certain dimension N x 2\n",
    "        where N varies. The first column contains the latitude points and the\n",
    "        second column contains the longitude points.\n",
    "        :param interval_size: Size of the reward intervals in dollars.\n",
    "        :param aggregate: Boolean variable to indicate whether to aggregate a \n",
    "        group of drivers if this field is true.\n",
    "        \"\"\"\n",
    "\n",
    "        self.driver_id = driver_id\n",
    "        self.neighborhoods = neighborhoods\n",
    "        self.interval_size = interval_size\n",
    "        self.change_pairs = change_pairs\n",
    "        self.ride_areas = ride_areas\n",
    "\n",
    "        # Dictionary to hold the mapping from area key to matrix index.\n",
    "        self.mapping = {self.ride_areas[v]: v for v in range(len(self.ride_areas))}\n",
    "        \n",
    "        # Dictionary to hold the mapping from matrix index to area key.\n",
    "        self.inverse_mapping = {v:k for k,v in self.mapping.items()}\n",
    "                \n",
    "        self.nodes = self.inverse_mapping.keys()\n",
    "        \n",
    "        if aggregate:\n",
    "            self.data = taxi_data.loc[taxi_data['hack_license'].isin(list(driver_id))]\n",
    "        else:\n",
    "            self.data = taxi_data.loc[taxi_data['hack_license'] == driver_id]\n",
    "            \n",
    "      #  self.data_ = self.data.copy()\n",
    "            \n",
    "        # Changing the areas to correspond to self.nodes.\n",
    "        self.data['start_trip_area'] = self.data['start_trip_area'].apply(lambda x: self.mapping[x])\n",
    "        self.data['end_trip_area'] = self.data['end_trip_area'].apply(lambda x: self.mapping[x])\n",
    "        self.data['prev_trip_area'] = self.data['prev_trip_area'].apply(lambda x: self.mapping[x])\n",
    "        \n",
    "        # Matrix to hold the probability of starting a trip in each area.\n",
    "        self.demand = np.zeros((len(self.nodes), 1))\n",
    "\n",
    "        self.trans_prob = np.zeros((len(self.nodes), len(self.nodes)))   \n",
    "\n",
    "        self.search_time = np.zeros((len(self.nodes), 1))\n",
    "        self.node_earn_rate = np.zeros((len(self.nodes), 1))\n",
    "        self.search_rewards = np.zeros((len(self.nodes), 1))\n",
    "\n",
    "        self.drive_time_avg = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "        self.drive_time_std = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "\n",
    "        self.fare_avg = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "        self.fare_std = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "\n",
    "        self.earn_rate_avg = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "        self.earn_rate_std = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "\n",
    "        self.full_reward_avg = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "        self.full_reward_std = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "\n",
    "        self.empty_drive_reward = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "        self.empty_reward_avg = np.zeros((len(self.nodes), len(self.nodes)))\n",
    "\n",
    "        self.empty_policy = []\n",
    "\n",
    "        # Median daily earnings value for driver or set of drivers.\n",
    "        self.ref = None\n",
    "\n",
    "        self.traj=None\n",
    "        self.T=30\n",
    "        self.state=None\n",
    "        self.N=None\n",
    "\n",
    "        self.states = []\n",
    "        self.final = []\n",
    "        self.state2num = {}\n",
    "        self.num2state = {}\n",
    "        self.X = []\n",
    "\n",
    "        self.actions = []\n",
    "        self.U = []\n",
    "        self.m = None\n",
    "        \n",
    "        self.reward_intervals = []\n",
    "\n",
    "        self.transitions = {}\n",
    "\n",
    "        self.rewards = {}\n",
    "\n",
    "        self.create_action_space()\n",
    "\n",
    "        self.create_state_space(aggregate)\n",
    "        \n",
    "        self.calculate_params()\n",
    "\n",
    "        # The initial state will always be the node with highest demand, meaning\n",
    "        # the most rides beginning in it, the taxi being empty, and the reward in interval [0,20).\n",
    "        self.initial = (max(enumerate(self.demand.tolist()), key=lambda x: x[1])[0], \n",
    "                              'e', self.reward_intervals[0])\n",
    "        \n",
    "        self.P = np.zeros((self.n, self.m, self.n))\n",
    "        self.R = np.zeros((self.n, self.m, self.n))\n",
    "        \n",
    "        self.get_trans_and_rewards()\n",
    "                        \n",
    "        self.check_probabilities()\n",
    "\n",
    "        del self.data\n",
    "        \n",
    "\n",
    "    def step(self,a):\n",
    "\n",
    "        self.traj.append([self.state,a])\n",
    "        self.N[self.state,a]+=1\n",
    "        self.state=int(np.random.choice(self.X,1,False,list(self.P[self.state,a,:])))\n",
    "\n",
    "        return int(self.state)\n",
    "\n",
    "\n",
    "    def initialize(self,state=0):\n",
    "\n",
    "        self.state=0\n",
    "        self.traj=[]\n",
    "        self.N=np.zeros((self.n,self.m))\n",
    "\n",
    "\n",
    "    def create_action_space(self):\n",
    "        \"\"\"Creating the complete action space for the MDP. \n",
    "        \n",
    "        The action space contains transitions from a location i to location j.\n",
    "        \n",
    "        :return: Nothing is returned but the class attribute self.actions is updated.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.actions = [i for i in self.nodes]\n",
    "        \n",
    "        self.U = self.actions\n",
    "        \n",
    "        self.m = len(self.U)\n",
    "        \n",
    "        \n",
    "    def create_state_space(self, aggregate):\n",
    "        \"\"\"Creating the full state space for the MDP.\n",
    "        \n",
    "        The complete state space is X = {N x S x R}\\X_na where N is the index\n",
    "        set of the zones or nodes in the city with N nodes, S = {e, f} are the\n",
    "        states indicating if the taxi is empty or full, and R is the discretized\n",
    "        cumulative fare value space. The states that are not allowed are \n",
    "        X_na = {(i, f, r)|r in R_terminal, i in N}.\n",
    "        \n",
    "        :param aggregate: Boolean variable for whether the MDP is over a set of \n",
    "        drivers or to aggregate for.\n",
    "\n",
    "        :return: Nothing is returned but the class attributes self.ref, \n",
    "        self.reward_intervals, and self.states are updated.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.get_ref(aggregate)\n",
    "        self.create_reward_intervals()\n",
    "        \n",
    "        i = 0\n",
    "        for state in itertools.product(self.nodes, ['e', 'f'], self.reward_intervals):\n",
    "            \n",
    "            # States that are not allowed.\n",
    "            if state[1] == 'f' and float('inf') in state[2]:\n",
    "                pass\n",
    "            else:\n",
    "                self.states.append(state)\n",
    "    \n",
    "                self.num2state[i] = state\n",
    "                self.state2num[state] = i\n",
    "                self.X.append(i)\n",
    "\n",
    "                if state[1] == 'e' and float('inf') in state[2]:\n",
    "                    self.final.append(i)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "        self.n = len(self.X)\n",
    "\n",
    "        \n",
    "    def get_ref(self, aggregate):  \n",
    "        \"\"\"Calculating the reference point by finding the median daily earnings.\n",
    "\n",
    "        :param aggregate: Boolean variable for whether the MDP is over a set of \n",
    "        drivers or to aggregate for.\n",
    "\n",
    "        :return: Nothing is returned but the class attribute is updated.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data['pickup_date'] = self.data['pickup_datetime'].apply(lambda x: x.date())\n",
    "        \n",
    "        dates = sorted(self.data['pickup_date'].unique())\n",
    "        \n",
    "        daily_earnings = []\n",
    "\n",
    "        if aggregate:\n",
    "            for date in dates:\n",
    "                driver_day = self.data.loc[self.data['pickup_date'] == date]\n",
    "                active_drivers = driver_day['hack_license'].unique()\n",
    "                daily_earnings.append(sum(driver_day['profit'])/float((len(active_drivers))))\n",
    "        else:\n",
    "            for date in dates:\n",
    "                driver_day = self.data.loc[self.data['pickup_date'] == date]\n",
    "                daily_earnings.append(sum(driver_day['profit']))\n",
    "\n",
    "        self.ref = np.median(np.array(daily_earnings)) \n",
    "        \n",
    "        \n",
    "    def create_reward_intervals(self):\n",
    "        \"\"\"Creating rewards intervals until the reference point.\n",
    "        \n",
    "        :return: Nothing is returned but self.reward_intervals is set.\n",
    "        \"\"\"\n",
    "        \n",
    "        reward = self.interval_size\n",
    "\n",
    "        while reward < self.ref:\n",
    "            self.reward_intervals.append((reward - self.interval_size, reward))\n",
    "            reward += self.interval_size\n",
    "\n",
    "        self.reward_intervals.append((reward - self.interval_size, self.ref))\n",
    "        \n",
    "        # This reward is the terminal reward state.\n",
    "        self.reward_intervals.append((self.ref, float('inf')))\n",
    "\n",
    "        \n",
    "    def calculate_params(self):\n",
    "        \"\"\"Getting parameters for the MDP.\n",
    "\n",
    "        :return: Nothing is returned but self.search_time.avg,\n",
    "        self.trans_prob, self.drive_time_avg,\n",
    "        self.drive_time.std, self.fare_avg, self.fare_std, self.earn_rate_avg,\n",
    "        self.earn_rate_std, self.full_reward_avg, self.full_reward_std, \n",
    "        and self.empty_reward_avg are all set.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        for start in sorted(self.nodes):\n",
    "            area_start = self.data.loc[self.data['start_trip_area'] == start]\n",
    "\n",
    "            self.demand[start] = len(area_start)/float(len(self.data))\n",
    "\n",
    "            t_search_trips = self.data.loc[(self.data['prev_trip_area'] == start)\n",
    "                                           & (self.data['start_trip_area'] == start)]\n",
    "            t_search_trips = t_search_trips.loc[t_search_trips['day_start'] == False]\n",
    "            t_search_trips = t_search_trips.loc[t_search_trips['seek_time'] <= 20]\n",
    "            \n",
    "            for end in sorted(self.nodes):\n",
    "                area_end = area_start.loc[area_start['end_trip_area'] == end]\n",
    "\n",
    "                if len(area_end) == 0:\n",
    "                    trans_prob = 0.0\n",
    "                    drive_avg = None\n",
    "                    drive_std = None\n",
    "                    fare_avg = 0.0\n",
    "                    fare_std = 0.0\n",
    "                    earn_rate_avg = None\n",
    "                    earn_rate_std = None\n",
    "                else:\n",
    "                    trans_prob = len(area_end)/float(len(area_start))\n",
    "                    drive_avg = area_end['trip_time'].mean()\n",
    "                    drive_std = area_end['trip_time'].std()\n",
    "                    fare_avg = area_end['profit'].mean()\n",
    "                    fare_std = area_end['profit'].std()\n",
    "                    earn_rate_avg = area_end['earn_rate'].mean()\n",
    "                    earn_rate_std = area_end['earn_rate'].std()\n",
    "\n",
    "                self.trans_prob[start, end] = trans_prob\n",
    "                \n",
    "                self.drive_time_avg[start, end] = drive_avg\n",
    "                self.drive_time_std[start, end] = drive_std\n",
    "                \n",
    "                self.fare_avg[start, end] = fare_avg\n",
    "                self.fare_std[start, end] = fare_std\n",
    "                \n",
    "                self.earn_rate_avg[start, end] = earn_rate_avg\n",
    "                self.earn_rate_std[start, end] = earn_rate_std\n",
    "                \n",
    "                self.full_reward_avg[start, end] = fare_avg\n",
    "                self.full_reward_std[start, end] = fare_std\n",
    "\n",
    "                if earn_rate_avg is not None:\n",
    "                    self.empty_drive_reward[start, end] = -drive_avg/float(earn_rate_avg**-1)\n",
    "                else:\n",
    "                    self.empty_drive_reward[start, end] = 0.0\n",
    "\n",
    "            if len(t_search_trips) == 0 or np.isnan(self.earn_rate_avg[start, start]) \\\n",
    "                or self.earn_rate_avg[start,start] == 0:\n",
    "\n",
    "                self.search_rewards[start] = 0\n",
    "            else:\n",
    "                self.search_rewards[start] = -t_search_trips['seek_time'].mean()/float(self.earn_rate_avg[start, start]**-1)\n",
    "\n",
    "\n",
    "        self.empty_search = np.where(self.search_rewards == 0)[0].tolist()\n",
    "        self.search_rewards[self.search_rewards == 0] = np.min(self.search_rewards)\n",
    "        \n",
    "        empty_drive = np.where(self.empty_drive_reward == 0.0)\n",
    "        empty_row_index = empty_drive[0]\n",
    "        empty_col_index = empty_drive[1]\n",
    "\n",
    "        self.empty_drive = [(empty_row_index[i], empty_col_index[i]) for i in range(len(empty_row_index))]\n",
    "        self.empty_drive_reward[self.empty_drive_reward == 0] = np.min(self.empty_drive_reward)\n",
    "\n",
    "        self.empty_reward_avg = self.empty_drive_reward + self.search_rewards.T\n",
    "\n",
    "        for node in sorted(self.nodes):\n",
    "            # In the case of the i to i transition, it is only the search time.\n",
    "            # This line is removing the empty_drive time reward.\n",
    "            self.empty_reward_avg[node, node] -= self.empty_drive_reward[node, node]\n",
    "    \n",
    "        \n",
    "    def get_trans_and_rewards(self):\n",
    "        \"\"\"Finding the transition probabilities and rewards for the MDP.\n",
    "\n",
    "        :return: Nothing is returned but self.transitions, self.P, self.rewards, \n",
    "        and self.R are updated.\n",
    "        \"\"\"\n",
    "        \n",
    "        for transition in itertools.product(self.states, self.actions, self.states):\n",
    "            state1 = transition[0]\n",
    "            action = transition[1]\n",
    "            state2 = transition[2]\n",
    "            \n",
    "            state_num1 = self.state2num[state1]\n",
    "            state_num2 = self.state2num[state2]\n",
    "\n",
    "            if state1[1] == 'e' and state2[1] == 'e':\n",
    "\n",
    "                # If in final state, you are guaranteed to stay there.\n",
    "                if float('inf') in state1[2] and float('inf') in state2[2] \\\n",
    "                                             and state1[0] == state2[0]:\n",
    "\n",
    "                    self.transitions[transition] = 1\n",
    "                    self.P[state_num1, action, state_num2] = 1\n",
    "                else:\n",
    "                    self.transitions[transition] = 0\n",
    "                    self.P[state_num1, action, state_num2] = 0\n",
    "                \n",
    "                # The reward is always 0 in this case.\n",
    "                self.rewards[transition] = 0\n",
    "                self.R[state_num1, action, state_num2] = 0\n",
    "\n",
    "            elif state1[1] == 'e' and state2[1] == 'f':\n",
    "\n",
    "                # This is case of driver picking someone up after a trip.\n",
    "                if state1[2] == state2[2] and float('inf') not in state1[2] and action == state2[0]:\n",
    "\n",
    "                    self.transitions[transition] = 1\n",
    "                    self.P[state_num1, action, state_num2] = 1\n",
    "\n",
    "                    self.rewards[transition] = self.empty_reward_avg[state1[0], state2[0]] \n",
    "                    self.R[state_num1, action, state_num2] = self.empty_reward_avg[state1[0], state2[0]]\n",
    "\n",
    "                else:\n",
    "                    self.transitions[transition] = 0\n",
    "                    self.P[state_num1, action, state_num2] = 0\n",
    "                    \n",
    "                    self.rewards[transition] = 0\n",
    "                    self.R[state_num1, action, state_num2] = 0\n",
    "\n",
    "            elif state1[1] == 'f' and state2[1] == 'f':\n",
    "                \n",
    "                # Never transition from full to full.\n",
    "                self.transitions[transition] = 0\n",
    "                self.P[state_num1, action, state_num2] = 0\n",
    "                \n",
    "                self.rewards[transition] = 0\n",
    "                self.R[state_num1, action, state_num2] = 0\n",
    "\n",
    "            elif state1[1] == 'f' and state2[1] == 'e':\n",
    "\n",
    "                # This is the case some fare is gained from the trip.\n",
    "                if state2[2][0] >= state1[2][0]:\n",
    "\n",
    "                    \"\"\"\n",
    "                    This piece of code is finding the probability: \n",
    "                    P((i,f,r), u, (j,f,r')) = P_dest(i,j)P(a_l - E[F(i,j)] <= r <= b_l - E[F(i,j)])\n",
    "                    The following code uses that r is assumed to be uniformly \n",
    "                    distributed on the interval of the reward state.\n",
    "                    \"\"\"\n",
    "\n",
    "                    trans_reward = state2[2]\n",
    "\n",
    "                    # Lower bound on reward being transitioned to.\n",
    "                    a_l = trans_reward[0]\n",
    "\n",
    "                    # Upper bound on reward being transitioned to.\n",
    "                    b_l = trans_reward[1]\n",
    "\n",
    "                    curr_reward = state1[2]\n",
    "\n",
    "                    # Lower bound on current reward.\n",
    "                    a_i = curr_reward[0]\n",
    "\n",
    "                    # Upper bound on current reward.\n",
    "                    b_i = curr_reward[1]\n",
    "\n",
    "                    start = state1[0] \n",
    "                    end = state2[0]\n",
    "\n",
    "                    p_dest = self.trans_prob[start, end]\n",
    "                    e_fare = self.fare_avg[start, end]\n",
    "\n",
    "                    x_1 = b_l - e_fare\n",
    "                    x_2 = a_l - e_fare\n",
    "\n",
    "                    # CDF of the upper bound.\n",
    "                    if x_1 < a_i:\n",
    "                        F_1 = 0\n",
    "                    elif x_1 < b_i:\n",
    "                        F_1 = (x_1 - a_i)/float(b_i - a_i)\n",
    "                    else:\n",
    "                        F_1 = 1\n",
    "\n",
    "                    # CDF of the lower bound.\n",
    "                    if x_2 < a_i:\n",
    "                        F_2 = 0\n",
    "                    elif x_2 < b_i:\n",
    "                        F_2 = (x_2 - a_i)/float(b_i - a_i)\n",
    "                    else:\n",
    "                        F_2 = 1\n",
    "\n",
    "                    p_reward = F_1 - F_2\n",
    "\n",
    "                    self.transitions[transition] = p_dest * p_reward\n",
    "                    self.P[state_num1, action, state_num2] = p_dest * p_reward\n",
    "                    \n",
    "                    if float('inf') in state2[2]:\n",
    "                        self.rewards[transition] = 1000\n",
    "                        self.R[state_num1, action, state_num2] = 1000\n",
    "                    else:\n",
    "                        self.rewards[transition] = self.full_reward_avg[state1[0], state2[0]]\n",
    "                        self.R[state_num1, action, state_num2] = self.full_reward_avg[state1[0], state2[0]]\n",
    "\n",
    "                else:\n",
    "                    self.transitions[transition] = 0   \n",
    "                    self.P[state_num1, action, state_num2] = 0\n",
    "                    \n",
    "                    self.rewards[transition] = 0\n",
    "                    self.R[state_num1, action, state_num2] = 0\n",
    "                    \n",
    "                    \n",
    "    def check_probabilities(self):\n",
    "        \"\"\"This function is to ensure that all probability functions are valid.\n",
    "        \n",
    "        To ensure the MDP is correct this function contains checks that each of\n",
    "        the probability functions are valid, for transitions and policies.\n",
    "        \n",
    "        :return: Nothing but assert error if probabilities are incorrect.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Checking valid density function for the transition probability.\n",
    "        for state in xrange(self.n):\n",
    "            for action in xrange(self.m):\n",
    "                assert abs(sum(self.P[state, action, :]) - 1) < 1e-3, 'Transitions do not sum to 1'\n",
    "\n",
    "        # Checking that there is no None values of any of the MDP.\n",
    "        assert True not in pd.isnull(self.P), 'None value in transitions'\n",
    "        assert True not in pd.isnull(self.R), 'None value in rewards'\n",
    "        assert True not in pd.isnull(self.U), 'None value in actions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), '..', 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods = pickle.load(open(os.path.join(data_dir, \"neighorhoods.p\"), \"rb\"))\n",
    "driver_areas = pickle.load(open(os.path.join(data_dir, \"driver_areas.p\"), \"rb\"))\n",
    "change_pairs = pickle.load(open(os.path.join(data_dir, \"change_pairs.p\"), \"rb\"))\n",
    "taxi_data = pd.read_csv(os.path.join(data_dir, 'taxi_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2010001271: {69: 7.3873873873873865,\n",
       "  70: 3.783783783783784,\n",
       "  74: 8.82882882882883,\n",
       "  75: 1.6666666666666667,\n",
       "  80: 29.72972972972973,\n",
       "  81: 12.117117117117116,\n",
       "  162: 7.072072072072072,\n",
       "  165: 18.423423423423422,\n",
       "  167: 3.6036036036036037,\n",
       "  171: 7.3873873873873865},\n",
       " 2010002704: {70: 6.103896103896104,\n",
       "  74: 9.35064935064935,\n",
       "  75: 2.4025974025974026,\n",
       "  80: 36.81818181818181,\n",
       "  81: 8.928571428571429,\n",
       "  155: 0.7142857142857143,\n",
       "  162: 11.720779220779221,\n",
       "  165: 13.376623376623375,\n",
       "  171: 9.285714285714286,\n",
       "  174: 1.2987012987012987},\n",
       " 2010002920: {69: 7.648351648351648,\n",
       "  70: 8.87912087912088,\n",
       "  74: 5.450549450549451,\n",
       "  80: 32.175824175824175,\n",
       "  81: 11.296703296703296,\n",
       "  83: 2.3736263736263736,\n",
       "  162: 14.68131868131868,\n",
       "  165: 10.593406593406593,\n",
       "  171: 5.4945054945054945,\n",
       "  174: 1.4065934065934065},\n",
       " 2010003240: {9: 10.889645114244045,\n",
       "  70: 12.348079727758872,\n",
       "  74: 3.3543996110841032,\n",
       "  80: 24.06417112299465,\n",
       "  81: 10.06319883325231,\n",
       "  83: 1.750121536217793,\n",
       "  162: 21.24453087019932,\n",
       "  165: 6.417112299465241,\n",
       "  171: 8.896451142440446,\n",
       "  174: 0.9722897423432183},\n",
       " 2010007519: {70: 14.074757729580064,\n",
       "  74: 5.676049838486387,\n",
       "  80: 27.503461005999075,\n",
       "  81: 10.844485463774804,\n",
       "  83: 2.9072450392247347,\n",
       "  153: 2.445777572681126,\n",
       "  162: 23.39640055376096,\n",
       "  165: 6.96815874480849,\n",
       "  171: 4.8454083987078915,\n",
       "  174: 1.338255652976465},\n",
       " 2010007579: {9: 17.28888888888889,\n",
       "  70: 4.666666666666667,\n",
       "  74: 9.777777777777779,\n",
       "  80: 25.377777777777776,\n",
       "  81: 10.177777777777777,\n",
       "  162: 11.777777777777777,\n",
       "  165: 11.422222222222222,\n",
       "  167: 3.5111111111111115,\n",
       "  171: 4.7555555555555555,\n",
       "  174: 1.2444444444444445},\n",
       " 2010007770: {9: 16.658801321378007,\n",
       "  69: 2.642756016989146,\n",
       "  70: 7.26757904672015,\n",
       "  74: 4.436054742803209,\n",
       "  80: 19.395941481831052,\n",
       "  81: 9.76875884851345,\n",
       "  153: 0.5191127890514393,\n",
       "  162: 24.209532798489857,\n",
       "  165: 7.6923076923076925,\n",
       "  171: 7.4091552619159975}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data, driver_areas, neighborhoods, change_pairs = load_data(dropbox_dir)\n",
    "\n",
    "for interval_size in [100]:\n",
    "    for driver in driver_areas.keys():\n",
    "        driver_mdp = MDP(driver, taxi_data, driver_areas[driver].keys(), \n",
    "                         neighborhoods, change_pairs, interval_size)\n",
    "\n",
    "        with open(os.path.join(dropbox_dir + '/data/MDP_SIM_NEW/', 'r' + str(interval_size) + \n",
    "                               '_driver_' + str(driver) + '.pkl'), 'wb') as f:\n",
    "\n",
    "            pickle.dump(driver_mdp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = sys.argv\n",
    "    argc = len(args)\n",
    "\n",
    "    # Insert path to dropbox location for riskTaxi.\n",
    "    tanner_dir = '/Users/tfiez/Dropbox/riskTaxi'\n",
    "\n",
    "    if args[1].lower() == 'tanner':\n",
    "        dropbox_dir = tanner_dir\n",
    "\n",
    "    if args[2] == 'default':\n",
    "        taxi_data, driver_areas, neighborhoods, change_pairs = load_data(dropbox_dir)\n",
    "\n",
    "        for interval_size in [100]:\n",
    "            for driver in driver_areas.keys():\n",
    "                driver_mdp = MDP(driver, taxi_data, driver_areas[driver].keys(), \n",
    "                                 neighborhoods, change_pairs, interval_size)\n",
    "\n",
    "                with open(os.path.join(dropbox_dir + '/data/MDP_SIM_NEW/', 'r' + str(interval_size) + \n",
    "                                       '_driver_' + str(driver) + '.pkl'), 'wb') as f:\n",
    "\n",
    "                    pickle.dump(driver_mdp, f)\n",
    "\n",
    "\n",
    "    elif args[2] == 'sample':\n",
    "\n",
    "        interval_size = 100\n",
    "\n",
    "        for samples in [100, 250, 500, 1000, 2500, 5000, 10000]:\n",
    "            taxi_data, driver_areas, neighborhoods, change_pairs = load_data(dropbox_dir, driver_ids=None,\n",
    "                                                                             aggregate=True, num_samples=samples)\n",
    "\n",
    "            taxi_data.to_csv(os.path.join(dropbox_dir + '/data/TAXI_DATA/', str(samples) + '_samples_taxi_data' + '.csv'), index=False)\n",
    "\n",
    "            driver = driver_areas.keys()[0]\n",
    "\n",
    "            driver_mdp = MDP(driver, taxi_data, driver_areas[driver].keys(), neighborhoods, change_pairs,\n",
    "                             interval_size, aggregate=True)\n",
    "\n",
    "            N = get_policy(mdp=driver_mdp, driver=tuple(driver_mdp.driver_id), data=taxi_data.copy())\n",
    "\n",
    "            np.savetxt(os.path.join(dropbox_dir + '/data/MDP_SIM_POLICY_V4/', 'r' + \n",
    "                       str(interval_size) + '_sample_' + str(samples) + '.csv'), N, delimiter=',')\n",
    "\n",
    "            with open(os.path.join(dropbox_dir + '/data/MDP_SIM_NEW_V4/', 'r' + \n",
    "                      str(interval_size) + '_sample_' + str(samples) + '.pkl'), 'wb') as f:\n",
    "                \n",
    "                pickle.dump(driver_mdp, f)\n",
    "\n",
    "    else:\n",
    "        print('Bad Input')\n",
    "        exit()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_counts(mdp, driver, data):\n",
    "    \"\"\"Finding the policy for the agent for node movement.\n",
    "\n",
    "    :param mdp: MDP object created for some set of taxi drivers.\n",
    "    :param driver: Driver id.\n",
    "    :param data: Data including the driver data.\n",
    "\n",
    "    :return: N, the policy for the driver.\n",
    "    \"\"\"\n",
    "\n",
    "    data = data.loc[data['hack_license'] == driver]\n",
    "\n",
    "    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n",
    "    data['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])\n",
    "\n",
    "    data['date'] = data['pickup_datetime'].apply(lambda x: x.date())\n",
    "\n",
    "    data['next_trip_area'] = data.groupby(['hack_license', 'date'])['start_trip_area'].shift(-1)\n",
    "\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Starting and ending areas of policy decision following each transaction.\n",
    "    data['start_choice'] = data['end_trip_area'].apply(lambda x: mdp.mapping[x])\n",
    "    data['end_choice'] = data['next_trip_area'].apply(lambda x: mdp.mapping[x])\n",
    "\n",
    "    N = np.zeros((mdp.m, mdp.m))\n",
    "\n",
    "    counts = data.groupby(['start_choice', 'end_choice'])['hack_license'].count()\n",
    "    counts = counts.reset_index(level=[0,1])\n",
    "\n",
    "    for start_node in mdp.nodes:\n",
    "        for end_node in mdp.nodes:\n",
    "            value = counts.loc[(counts['start_choice'] == start_node) & (counts['end_choice'] == end_node)]['hack_license'].values\n",
    "            if not value:\n",
    "                N[start_node, end_node] = 0\n",
    "            else:\n",
    "                N[start_node, end_node] = value[0]\n",
    "\n",
    "    return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(mdp, driver=None, data=None):\n",
    "    \"\"\"Finding the policy for the agent.\n",
    "\n",
    "    :param mdp: MDP object created for some set of taxi drivers.\n",
    "    :param driver: Driver id.\n",
    "    :param data: Data including the driver data.\n",
    "\n",
    "    :return: N, the policy for the driver.\n",
    "    \"\"\"\n",
    "\n",
    "    # Case in which the mdp contains the data for all the driver ids.\n",
    "    if data is None and driver is None:\n",
    "        data = mdp.data_\n",
    "        driver = mdp.driver_id\n",
    "\n",
    "    # Case in which the mdp contains the data for the drivers being selected.\n",
    "    elif data is None and driver is not None:\n",
    "        data = mdp.data_\n",
    "\n",
    "    # Case in which we want policy for drivers that were not used in making the mdp.\n",
    "    elif data is not None and driver is not None:\n",
    "        pass\n",
    "\n",
    "    if isinstance(driver, tuple):\n",
    "        data = data.loc[data['hack_license'].isin(driver)]\n",
    "    else:\n",
    "        data = data.loc[data['hack_license'] == driver]\n",
    "\n",
    "    data['date'] = data['pickup_datetime'].apply(lambda x: x.date())\n",
    "\n",
    "    data['cum_rewards'] = pd.Series([None for row in xrange(len(data))], index=data.index)\n",
    "\n",
    "    # Tracking the daily cumulative rewards at each transaction.\n",
    "    data['cum_rewards'] = data.groupby(['hack_license', 'date'])['profit'].cumsum()\n",
    "\n",
    "    # Label indicating what reward interval earnings are at following a transaction.\n",
    "    data['reward_interval'] = data['cum_rewards'].apply(lambda y: mdp.reward_intervals.index(filter(lambda x: x[0] <= y < x[1], \n",
    "                                                                                                     mdp.reward_intervals)[0]))\n",
    "\n",
    "    data['next_trip_area'] = data.groupby(['hack_license', 'date'])['start_trip_area'].shift(-1)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Starting and ending areas of policy decision following each transaction.\n",
    "    data['start_choice'] = data['end_trip_area'].apply(lambda x: mdp.mapping[x])\n",
    "    data['end_choice'] = data['next_trip_area'].apply(lambda x: mdp.mapping[x])\n",
    "\n",
    "    # Finding the policy for the data.\n",
    "    N = np.zeros((mdp.n, mdp.m))\n",
    "\n",
    "    for state in mdp.states:\n",
    "        state_num = mdp.state2num[state]\n",
    "\n",
    "        # Empty and not in final reward indicates a choice is being made.\n",
    "        if state[1] == 'e' and state[2] != mdp.reward_intervals[-1]:\n",
    "\n",
    "            state_num = mdp.state2num[state]\n",
    "\n",
    "            start_choice = state[0]\n",
    "\n",
    "            reward_interval = mdp.reward_intervals.index(state[2])\n",
    "\n",
    "            final_reward = mdp.reward_intervals.index(mdp.reward_intervals[-1])\n",
    "\n",
    "            for action in mdp.actions:\n",
    "                N[state_num, action] = len(data.loc[(data['reward_interval'] == reward_interval) & \n",
    "                                                   (data['start_choice'] == start_choice) & \n",
    "                                                   (data['end_choice'] == action) & \n",
    "                                                   (data['reward_interval'] != final_reward)])\n",
    "        else:\n",
    "            N[state_num, :] = 1/float(len(mdp.actions))\n",
    "\n",
    "    empty_rows = np.where(~N.any(axis=1))[0].tolist()\n",
    "\n",
    "    if not empty_rows:\n",
    "        pass\n",
    "    else:\n",
    "        for row in empty_rows:\n",
    "            N[row] = 1\n",
    "\n",
    "    return N"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
